{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "New_Data_Processing_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# unzipping the file contents into dataset folder\n",
        "!wget https://personal.utdallas.edu/~pxn210006/cnn_dailymail/train.zip\n",
        "!unzip /content/train.zip -d /content/dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nN-mWtcyTZSw",
        "outputId": "a668320d-ee1f-4d6d-e974-28fd3d290669"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-28 00:21:07--  https://personal.utdallas.edu/~pxn210006/cnn_dailymail/train.zip\n",
            "Resolving personal.utdallas.edu (personal.utdallas.edu)... 129.110.46.112\n",
            "Connecting to personal.utdallas.edu (personal.utdallas.edu)|129.110.46.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 479197291 (457M) [application/zip]\n",
            "Saving to: ‘train.zip’\n",
            "\n",
            "train.zip           100%[===================>] 457.00M  3.76MB/s    in 2m 2s   \n",
            "\n",
            "2022-04-28 00:23:14 (3.74 MB/s) - ‘train.zip’ saved [479197291/479197291]\n",
            "\n",
            "Archive:  /content/train.zip\n",
            "  inflating: /content/dataset/train.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# installing spark in colab and creating spark session\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "findspark.find()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "TJWUkckK2Ofi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading csv dataset file in spark dataframe\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
        "dataset_schema = StructType([ \\\n",
        "    StructField(\"text\",StringType(),True), \\\n",
        "    StructField(\"summary\",StringType(),True), \\\n",
        "  ])\n",
        "train_df = spark.read.csv(\"/content/dataset/train.csv\", schema = dataset_schema)\n",
        "train_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJDt2xa645LV",
        "outputId": "15cef82e-4426-4627-8640-78c1736c11da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- summary: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# showing dataset\n",
        "train_df = train_df.na.drop()\n",
        "train_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSX0S-Vp8SjN",
        "outputId": "299b71a4-ccd6-48e5-a4ba-3ceb3c13ac9a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|                text|             summary|\n",
            "+--------------------+--------------------+\n",
            "|                  id|             article|\n",
            "|0001d1afc246a7964...|By . Associated P...|\n",
            "|Church members in...| Grand Forks and ...|\n",
            "|0002095e55fcbd3a2...|\"(CNN) -- Ralph M...|\n",
            "|          Ralph Mata| an internal affa...|\n",
            "|He also arranged ...| a complaint alle...|\n",
            "|00027e965c8264c35...|A drunk driver wh...|\n",
            "|Was using phone w...|     Isle of Wight .|\n",
            "|Crashed head-on i...| who died in hosp...|\n",
            "|0002c17436637c4fe...|(CNN) -- With a b...|\n",
            "|Targeting Russia'...|          she says .|\n",
            "|0003ad6ef0c37534f...|Fleetwood are the...|\n",
            "|        Peterborough|        Bristol City|\n",
            "|0004306354494f090...|He's been accused...|\n",
            "|0005d61497d21ff37...|By . Daily Mail R...|\n",
            "|0006021f772fad0aa...|\"By . Daily Mail ...|\n",
            "|Passenger Chris D...|                  46|\n",
            "|00083697263e215e5...|There are a numbe...|\n",
            "|000940f2bb357ac04...|\"Canberra, Austra...|\n",
            "|Even if the fligh...| information is r...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filtering long and shorter texts\n",
        "from  pyspark.sql.functions import *\n",
        "\n",
        "max_art_len = 520\n",
        "min_art_len = 20\n",
        "max_sum_len = 30\n",
        "\n",
        "train_df = train_df.filter(length(col(\"Summary\")) <= max_sum_len)\n",
        "train_df = train_df.filter(length(col(\"Text\")) >= min_art_len)\n",
        "train_df = train_df.filter(length(col(\"Text\")) <= max_art_len)\n",
        "train_df.show()"
      ],
      "metadata": {
        "id": "HepYWvRn7SK9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6a44d74-524e-46fe-f9a9-8bfd312f9218"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|                text|             summary|\n",
            "+--------------------+--------------------+\n",
            "|He also arranged ...| a complaint alle...|\n",
            "|Was using phone w...|     Isle of Wight .|\n",
            "|Crashed head-on i...| who died in hosp...|\n",
            "|Targeting Russia'...|          she says .|\n",
            "|Passenger Chris D...|                  46|\n",
            "|Even if the fligh...| information is r...|\n",
            "|stepping up Speci...| including Britis...|\n",
            "|Pennsylvania had ...| with Ohio a clos...|\n",
            "|China kept the vi...|      Xinhua said .\"|\n",
            "|Anti-Morsy politi...| according to sta...|\n",
            "|Maryellen followe...|        California .|\n",
            "|Authorities say M...|                  20|\n",
            "|\"\"Rebel tours\"\" w...|          unofficial|\n",
            "|Journalists were ...|                beat|\n",
            "|The rift between ...|          it says .\"|\n",
            "|Terrorists with b...| panel's chairman...|\n",
            "|Biological attack...|       report says .|\n",
            "|Number of nations...|       panel says .\"|\n",
            "|He stabbed fiancé...|                 arm|\n",
            "|He and pilot then...|  personal computers|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cleaning text\n",
        "import string\n",
        "def func_text_cleaner(text):\n",
        "    text = text.lower()\n",
        "    printable = set(string.printable)\n",
        "    text = \"\".join(list(filter(lambda x: x in printable, text)))\n",
        "    return text\n",
        "\n",
        "columns = ['article', 'summary']\n",
        "cleaned_dataset = train_df.rdd.map(lambda X:func_text_cleaner(X[0])+func_text_cleaner(X[1]))\n",
        "cleaned_dataset = train_df.rdd.toDF(columns)\n",
        "cleaned_dataset.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XM1pt1qYohT",
        "outputId": "456d87bd-7927-4daa-abe8-93ea4af5bbd4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|             article|             summary|\n",
            "+--------------------+--------------------+\n",
            "|He also arranged ...| a complaint alle...|\n",
            "|Was using phone w...|     Isle of Wight .|\n",
            "|Crashed head-on i...| who died in hosp...|\n",
            "|Targeting Russia'...|          she says .|\n",
            "|Passenger Chris D...|                  46|\n",
            "|Even if the fligh...| information is r...|\n",
            "|stepping up Speci...| including Britis...|\n",
            "|Pennsylvania had ...| with Ohio a clos...|\n",
            "|China kept the vi...|      Xinhua said .\"|\n",
            "|Anti-Morsy politi...| according to sta...|\n",
            "|Maryellen followe...|        California .|\n",
            "|Authorities say M...|                  20|\n",
            "|\"\"Rebel tours\"\" w...|          unofficial|\n",
            "|Journalists were ...|                beat|\n",
            "|The rift between ...|          it says .\"|\n",
            "|Terrorists with b...| panel's chairman...|\n",
            "|Biological attack...|       report says .|\n",
            "|Number of nations...|       panel says .\"|\n",
            "|He stabbed fiancé...|                 arm|\n",
            "|He and pilot then...|  personal computers|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing text into individual words\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "\n",
        "article_tokenizer = Tokenizer(inputCol=\"article\", outputCol=\"article_words\") \n",
        "summary_tokenizer = Tokenizer(inputCol=\"summary\", outputCol=\"summary_words\")\n",
        "\n",
        "tokenized_dataset = article_tokenizer.transform(cleaned_dataset)\n",
        "tokenized_dataset = summary_tokenizer.transform(tokenized_dataset)\n",
        "\n",
        "tokenized_dataset.show()"
      ],
      "metadata": {
        "id": "kbu5ld71ZSt-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ff78fe-ebfb-4486-8f61-7e36041989e3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|             article|             summary|       article_words|       summary_words|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|He also arranged ...| a complaint alle...|[he, also, arrang...|[, a, complaint, ...|\n",
            "|Was using phone w...|     Isle of Wight .|[was, using, phon...|[, isle, of, wigh...|\n",
            "|Crashed head-on i...| who died in hosp...|[crashed, head-on...|[, who, died, in,...|\n",
            "|Targeting Russia'...|          she says .|[targeting, russi...|    [, she, says, .]|\n",
            "|Passenger Chris D...|                  46|[passenger, chris...|              [, 46]|\n",
            "|Even if the fligh...| information is r...|[even, if, the, f...|[, information, i...|\n",
            "|stepping up Speci...| including Britis...|[stepping, up, sp...|[, including, bri...|\n",
            "|Pennsylvania had ...| with Ohio a clos...|[pennsylvania, ha...|[, with, ohio, a,...|\n",
            "|China kept the vi...|      Xinhua said .\"|[china, kept, the...|[, xinhua, said, .\"]|\n",
            "|Anti-Morsy politi...| according to sta...|[anti-morsy, poli...|[, according, to,...|\n",
            "|Maryellen followe...|        California .|[maryellen, follo...|   [, california, .]|\n",
            "|Authorities say M...|                  20|[authorities, say...|              [, 20]|\n",
            "|\"\"Rebel tours\"\" w...|          unofficial|[\"\"rebel, tours\"\"...|      [, unofficial]|\n",
            "|Journalists were ...|                beat|[journalists, wer...|            [, beat]|\n",
            "|The rift between ...|          it says .\"|[the, rift, betwe...|    [, it, says, .\"]|\n",
            "|Terrorists with b...| panel's chairman...|[terrorists, with...|[, panel's, chair...|\n",
            "|Biological attack...|       report says .|[biological, atta...| [, report, says, .]|\n",
            "|Number of nations...|       panel says .\"|[number, of, nati...| [, panel, says, .\"]|\n",
            "|He stabbed fiancé...|                 arm|[he, stabbed, fia...|             [, arm]|\n",
            "|He and pilot then...|  personal computers|[he, and, pilot, ...|[, personal, comp...|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a vocabulary to index mapping which contains all words from summary and articles\n",
        "tokenized_dataset = tokenized_dataset.select(\"article_words\", \"summary_words\")\n",
        "tokenized_dataset = tokenized_dataset.dropna()\n",
        "\n",
        "tokenized_data = tokenized_dataset.collect()\n",
        "vocabulary_index_mapping = {}\n",
        "\n",
        "for row in tokenized_data:\n",
        "  for word in row[\"article_words\"]:\n",
        "      if word not in vocabulary_index_mapping:\n",
        "          vocabulary_index_mapping[word]=len(vocabulary_index_mapping)\n",
        "          \n",
        "  for word in row[\"summary_words\"]:\n",
        "      if word not in vocabulary_index_mapping:\n",
        "          vocabulary_index_mapping[word]=len(vocabulary_index_mapping)\n",
        "\n",
        "print(vocabulary_index_mapping['is'])"
      ],
      "metadata": {
        "id": "C5Z_Iio6bwGQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3085cc97-7aea-41ac-f560-43df79e7db4f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dowloading pre trained GloVe word embedding file\n",
        "!wget https://personal.utdallas.edu/~pxn210006/cnn_dailymail/glove.zip\n",
        "!unzip glove.zip -d /content/glove_word_embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "to7zjNYaWAhZ",
        "outputId": "2253c154-40fd-449c-bfc0-ea4485641047"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-28 00:26:23--  https://personal.utdallas.edu/~pxn210006/cnn_dailymail/glove.zip\n",
            "Resolving personal.utdallas.edu (personal.utdallas.edu)... 129.110.46.112\n",
            "Connecting to personal.utdallas.edu (personal.utdallas.edu)|129.110.46.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 137847651 (131M) [application/zip]\n",
            "Saving to: ‘glove.zip’\n",
            "\n",
            "glove.zip           100%[===================>] 131.46M  3.78MB/s    in 35s     \n",
            "\n",
            "2022-04-28 00:26:58 (3.78 MB/s) - ‘glove.zip’ saved [137847651/137847651]\n",
            "\n",
            "Archive:  glove.zip\n",
            "  inflating: /content/glove_word_embeddings/glove.6B.100d.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating mapping of word and its embedding vector\n",
        "import numpy as np\n",
        "\n",
        "def load_word_embeddings(glove_file):\n",
        "  vocabulary_embedding_mapping = {}\n",
        "\n",
        "  with open(glove_file) as g_file:     \n",
        "    for line in g_file:\n",
        "        row = line.strip().split(' ')\n",
        "        word = row[0].lower()\n",
        "        if word not in vocabulary_embedding_mapping:\n",
        "            vocabulary_embedding_mapping[word] = np.asarray(row[1:], np.float32)\n",
        "  \n",
        "  return vocabulary_embedding_mapping;\n",
        "\n",
        "vocabulary_embedding_mapping = load_word_embeddings('/content/glove_word_embeddings/glove.6B.100d.txt')"
      ],
      "metadata": {
        "id": "dY3Pj06vnYTa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# building vocabulary and its embeddings\n",
        "vocabulary_words = []\n",
        "embeddings_word = []\n",
        "special_tags = ['<UNK>','<PAD>','<EOS>']\n",
        "\n",
        "for word in vocabulary_index_mapping:\n",
        "    if word in vocabulary_embedding_mapping:\n",
        "        vocabulary_words.append(word)\n",
        "        embeddings_word.append(vocabulary_embedding_mapping[word])\n",
        "        \n",
        "for special_tag in special_tags:\n",
        "    vocabulary_words.append(special_tag)\n",
        "    embeddings_word.append(np.random.rand(len(embeddings_word[0]),))\n",
        "    \n",
        "vocabulary_index_mapping = {word:idx for idx,word in enumerate(vocabulary_words)}\n",
        "embeddings_word = np.asarray(embeddings_word, np.float32)\n",
        "\n",
        "print(\"Size of vacabulary: {}\".format(len(vocabulary_index_mapping)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1m1vejZoqPO",
        "outputId": "7645acf4-8750-44b9-e1d8-22596d53e210"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vacabulary: 45395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build word vector for articles and summaries\n",
        "articles_vector = []\n",
        "summaries_vector = []\n",
        "\n",
        "unk_vector = vocabulary_index_mapping['<UNK>']\n",
        "for row in tokenized_data:\n",
        "    # Replace out of vocab words with index for '<UNK>' tag\n",
        "    article_vector = []\n",
        "    summary_vector = []\n",
        "\n",
        "    for word in row[\"article_words\"]:\n",
        "      article_vector.append(vocabulary_index_mapping.get(word, unk_vector))\n",
        "    for word in row[\"summary_words\"]:\n",
        "      summary_vector.append(vocabulary_index_mapping.get(word, unk_vector))\n",
        "    \n",
        "    articles_vector.append(article_vector)\n",
        "    summaries_vector.append(summary_vector)\n",
        "\n",
        "print(article_vector)"
      ],
      "metadata": {
        "id": "Of3z3uvEUjTG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9ec61b7-7eba-456a-cccc-2f38cf201c04"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[55, 2624, 114, 3663, 220, 662, 253, 289, 376]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly shuffle the data\n",
        "import random\n",
        "random.seed(101)\n",
        "\n",
        "article_indexes = [index for index in range(len(articles_vector))]\n",
        "random.shuffle(article_indexes)\n",
        "\n",
        "articles_vector = [articles_vector[index] for index in article_indexes]\n",
        "summaries_vector = [summaries_vector[index] for index in article_indexes]"
      ],
      "metadata": {
        "id": "F435ozCcUluh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use first 10000 data for testing, the next 10000 data for validation, and rest for training\n",
        "sum_vec = len(summaries_vector)\n",
        "art_vec = len(summaries_vector)\n",
        "test_data_summaries = summaries_vector[0:sum_vec//10]\n",
        "test_data_articles = articles_vector[0:art_vec // 10]\n",
        "\n",
        "validation_data_summaries = summaries_vector[sum_vec//10:sum_vec//5]\n",
        "validation_data_articles = articles_vector[art_vec//10:art_vec//5]\n",
        "\n",
        "training_data_summaries = summaries_vector[sum_vec//5:]\n",
        "training_data_articles = articles_vector[art_vec//5:]"
      ],
      "metadata": {
        "id": "_EgjnodTUoNZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creates batch of articles and summaries\n",
        "def create_batch(articles, summaries, dataset_batch_size=32):\n",
        "    \n",
        "    article_lengths = [len(article) for article in articles]\n",
        "    sorted_indexes = np.flip(np.argsort(article_lengths),axis=0)\n",
        "    articles = [articles[index] for index in sorted_indexes]\n",
        "    summaries = [summaries[index] for index in sorted_indexes]\n",
        "    \n",
        "    article_batches = []\n",
        "    summary_batches = []\n",
        "    article_batches_article_length = []\n",
        "    summary_batches_summary_length = []\n",
        "    \n",
        "    i = 0\n",
        "    while i < (len(articles)-dataset_batch_size):\n",
        "        \n",
        "        max_article_length = len(articles[i])\n",
        "        \n",
        "        article_batch = []\n",
        "        summary_batch = []\n",
        "        article_batch_article_length = []\n",
        "        summary_batch_summary_length = []\n",
        "        \n",
        "        for j in range(dataset_batch_size):\n",
        "            \n",
        "            padded_article = articles[i+j]\n",
        "            padded_summary = summaries[i+j]\n",
        "            \n",
        "            article_batch_article_length.append(len(articles[i+j]))\n",
        "            summary_batch_summary_length.append(len(summaries[i+j])+1)\n",
        "     \n",
        "            while len(padded_article) < max_article_length:\n",
        "                padded_article.append(vocabulary_index_mapping['<PAD>'])\n",
        "\n",
        "            padded_summary.append(vocabulary_index_mapping['<EOS>']) #End of Sentence Marker\n",
        "            while len(padded_summary) < max_sum_len+1:\n",
        "                padded_summary.append(vocabulary_index_mapping['<PAD>'])\n",
        "            \n",
        "        \n",
        "            article_batch.append(padded_article)\n",
        "            summary_batch.append(padded_summary)\n",
        "        \n",
        "        article_batches.append(article_batch)\n",
        "        summary_batches.append(summary_batch)\n",
        "        article_batches_article_length.append(article_batch_article_length)\n",
        "        summary_batches_summary_length.append(summary_batch_summary_length)\n",
        "        \n",
        "        i += dataset_batch_size\n",
        "        \n",
        "    return article_batches, summary_batches, article_batches_article_length, summary_batches_summary_length"
      ],
      "metadata": {
        "id": "-jaGa4LQUtJK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting batches for train, test and validation\n",
        "train_article_batches, train_summary_batches, train_article_batches_article_length, train_summary_batches_summary_length \\\n",
        "= create_batch(training_data_articles, training_data_summaries)\n",
        "\n",
        "validation_article_batches, validation_summary_batches, validation_article_batches_article_length, validation_summary_batches_summary_length \\\n",
        "= create_batch(validation_data_articles, validation_data_summaries)\n",
        "\n",
        "test_article_batches, test_summary_batches, test_article_batches_article_length, test_summary_batches_summary_length \\\n",
        "= create_batch(test_data_articles, test_data_summaries)"
      ],
      "metadata": {
        "id": "uuRNoh-xUvWc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading all batches and mapping dictionary to json file\n",
        "import json\n",
        "\n",
        "final_dictionary = {}\n",
        "\n",
        "final_dictionary[\"vocabulary_index_mapping\"] = vocabulary_index_mapping\n",
        "final_dictionary[\"embeddings_word\"] = embeddings_word.tolist()\n",
        "final_dictionary[\"train_article_batches\"] = train_article_batches\n",
        "final_dictionary[\"test_article_batches\"] = test_article_batches\n",
        "final_dictionary[\"validation_article_batches\"] = validation_article_batches\n",
        "final_dictionary[\"train_summary_batches\"] = train_summary_batches\n",
        "final_dictionary[\"test_summary_batches\"] = test_summary_batches\n",
        "final_dictionary[\"validation_summary_batches\"] = validation_summary_batches\n",
        "final_dictionary[\"train_article_batches_article_length\"] = train_article_batches_article_length\n",
        "final_dictionary[\"validation_article_batches_article_length\"] = validation_article_batches_article_length\n",
        "final_dictionary[\"test_article_batches_article_length\"] = test_article_batches_article_length\n",
        "final_dictionary[\"train_summary_batches_summary_length\"] = train_summary_batches_summary_length\n",
        "final_dictionary[\"validation_summary_batches_summary_length\"] = validation_summary_batches_summary_length\n",
        "final_dictionary[\"test_summary_batches_summary_length\"] = test_summary_batches_summary_length\n",
        "\n",
        "with open('CNN_Dailymail_Data.json', 'w') as output_file:\n",
        "    json.dump(final_dictionary, output_file)"
      ],
      "metadata": {
        "id": "cV0x_kQbUxva"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}