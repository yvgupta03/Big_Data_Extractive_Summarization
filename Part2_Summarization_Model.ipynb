{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBmE0uQsXpzw",
        "outputId": "c35817df-e33f-4638-e443-f3b9341243dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-27 22:45:08--  https://personal.utdallas.edu/~pxn210006/cnn_dailymail/CNN_Dailymail_Data.zip\n",
            "Resolving personal.utdallas.edu (personal.utdallas.edu)... 129.110.46.112\n",
            "Connecting to personal.utdallas.edu (personal.utdallas.edu)|129.110.46.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40060395 (38M) [application/zip]\n",
            "Saving to: ‘CNN_Dailymail_Data.zip’\n",
            "\n",
            "CNN_Dailymail_Data. 100%[===================>]  38.20M   832KB/s    in 49s     \n",
            "\n",
            "2022-04-27 22:45:58 (804 KB/s) - ‘CNN_Dailymail_Data.zip’ saved [40060395/40060395]\n",
            "\n",
            "Archive:  /content/CNN_Dailymail_Data.zip\n",
            "  inflating: /content/json_data/CNN_Dailymail_Data.json  \n"
          ]
        }
      ],
      "source": [
        "!wget https://personal.utdallas.edu/~pxn210006/cnn_dailymail/CNN_Dailymail_Data.zip\n",
        "!unzip /content/CNN_Dailymail_Data.zip -d /content/json_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHU2-15SW7lY"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('/content/json_data/CNN_Dailymail_Data.json') as dataFile:\n",
        "\n",
        "    for cnnJsonData in dataFile:\n",
        "        processedData = json.loads(cnnJsonData)\n",
        "\n",
        "        vocabulary_index_mapping = processedData[\"vocabulary_index_mapping\"]\n",
        "        embd = processedData[\"embeddings_word\"]\n",
        "        train_article_batches = processedData[\"train_article_batches\"]\n",
        "        test_article_batches = processedData[\"test_article_batches\"]\n",
        "        validation_article_batches = processedData[\"validation_article_batches\"]\n",
        "        train_summary_batches = processedData[\"train_summary_batches\"]\n",
        "        test_summary_batches = processedData[\"test_summary_batches\"]\n",
        "        validation_summary_batches= processedData[\"validation_summary_batches\"]\n",
        "        train_article_batches_article_length = processedData[\"train_article_batches_article_length\"]\n",
        "        validation_article_batches_article_length = processedData[\"validation_article_batches_article_length\"]\n",
        "        test_article_batches_article_length = processedData[\"test_article_batches_article_length\"]\n",
        "        train_summary_batches_summary_length = processedData[\"train_summary_batches_summary_length\"]\n",
        "        validation_summary_batches_summary_length = processedData[\"validation_summary_batches_summary_length\"]\n",
        "        test_summary_batches_summary_length = processedData[\"test_summary_batches_summary_length\"]\n",
        "\n",
        "        break\n",
        "        \n",
        "index_vocabulary_mapping = {vocabValue:vocabKey for vocabKey,vocabValue in vocabulary_index_mapping.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge-UKiaKX_aM"
      },
      "outputs": [],
      "source": [
        "# All hyper parameters \n",
        "hid_lay_cnn = 300\n",
        "learnRate_cnn = 0.001\n",
        "epochs = 1\n",
        "MaxSummaryLen_CNN = 31 \n",
        "Attention_Dimension_Size = 5 \n",
        "winDowCNNSize = 2*Attention_Dimension_Size+1\n",
        "regL2Param=1e-6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuJWMdAGYJcU",
        "outputId": "03128720-cb8c-46e6-e09b-7371e0347032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "#loading tensorflow\n",
        "import tensorflow.compat.v1 as version1_compat_tensorflow \n",
        "version1_compat_tensorflow.reset_default_graph()\n",
        "version1_compat_tensorflow.disable_v2_behavior()\n",
        "version1_compat_tensorflow.disable_eager_execution()\n",
        "\n",
        "embedding_dimensions = len(embd[0])\n",
        "\n",
        "article_tensor_cnn = version1_compat_tensorflow.placeholder(version1_compat_tensorflow.int32, [None, None])\n",
        "embed_tensor_cnn = version1_compat_tensorflow.placeholder(version1_compat_tensorflow.float32, [len(vocabulary_index_mapping),embedding_dimensions])\n",
        "summary_true_tensor_len_cnn = version1_compat_tensorflow.placeholder(version1_compat_tensorflow.int32, [None])\n",
        "sumCNN_Tensor = version1_compat_tensorflow.placeholder(version1_compat_tensorflow.int32,[None, None])\n",
        "train_tensor_cnn = version1_compat_tensorflow.placeholder(version1_compat_tensorflow.bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bktK3iVNYL0L"
      },
      "outputs": [],
      "source": [
        "#defining dropout function \n",
        "def CnnDropout(x,rate,training):\n",
        "    return version1_compat_tensorflow.cond(train_tensor_cnn,\n",
        "                    lambda: version1_compat_tensorflow.nn.dropout(x,rate=0.3),\n",
        "                    lambda: x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaGsTCEMYPxt"
      },
      "outputs": [],
      "source": [
        "# creating embedding lookup and getting dropout\n",
        "CnnEmbeddedSummary = version1_compat_tensorflow.nn.embedding_lookup(embed_tensor_cnn, article_tensor_cnn)\n",
        "\n",
        "CnnEmbeddedSummary = CnnDropout(CnnEmbeddedSummary,rate=0.3,training=train_tensor_cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AKv1eqQYRrU"
      },
      "outputs": [],
      "source": [
        "# defining the lstm model architecture with all hyper parameters\n",
        "def CnnLstm(x,CnnHiddenState,CLNcell,CnninputDim,hid_lay_cnn,scope):\n",
        "    \n",
        "    with version1_compat_tensorflow.variable_scope(scope,reuse=version1_compat_tensorflow.AUTO_REUSE):\n",
        "        \n",
        "        CnnLstmWeight = version1_compat_tensorflow.get_variable(\"w\", shape=[4,CnninputDim,hid_lay_cnn],\n",
        "                                    dtype=version1_compat_tensorflow.float32,\n",
        "                                    trainable=True,\n",
        "                                    initializer=version1_compat_tensorflow.glorot_uniform_initializer())\n",
        "        \n",
        "        CnnLstmU = version1_compat_tensorflow.get_variable(\"u\", shape=[4,hid_lay_cnn,hid_lay_cnn],\n",
        "                            dtype=version1_compat_tensorflow.float32,\n",
        "                            trainable=True,\n",
        "                            initializer=version1_compat_tensorflow.glorot_uniform_initializer())\n",
        "        \n",
        "        CnnLstmB = version1_compat_tensorflow.get_variable(\"bias\", shape=[4,1,hid_lay_cnn],\n",
        "                    dtype=version1_compat_tensorflow.float32,\n",
        "                    trainable=True,\n",
        "                    initializer=version1_compat_tensorflow.zeros_initializer())\n",
        "        \n",
        "    CnnGateInput= version1_compat_tensorflow.nn.sigmoid( version1_compat_tensorflow.matmul(x,CnnLstmWeight[0]) + version1_compat_tensorflow.matmul(CnnHiddenState,CnnLstmU[0]) + CnnLstmB[0])\n",
        "    CnnGateForget = version1_compat_tensorflow.nn.sigmoid( version1_compat_tensorflow.matmul(x,CnnLstmWeight[1]) + version1_compat_tensorflow.matmul(CnnHiddenState,CnnLstmU[1]) + CnnLstmB[1])\n",
        "    CnnGateOutput = version1_compat_tensorflow.nn.sigmoid( version1_compat_tensorflow.matmul(x,CnnLstmWeight[2]) + version1_compat_tensorflow.matmul(CnnHiddenState,CnnLstmU[2]) + CnnLstmB[2])\n",
        "    CnnCell_ = version1_compat_tensorflow.nn.tanh( version1_compat_tensorflow.matmul(x,CnnLstmWeight[3]) + version1_compat_tensorflow.matmul(CnnHiddenState,CnnLstmU[3]) + CnnLstmB[3])\n",
        "    CnnCell = CnnGateForget*CLNcell + CnnGateInput*CnnCell_\n",
        "    CnnHiddenState = CnnGateOutput*version1_compat_tensorflow.tanh(CnnCell)\n",
        "    \n",
        "    return CnnHiddenState, CnnCell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDBVIMfoYU3L"
      },
      "outputs": [],
      "source": [
        "# defining architecture for forward encoding\n",
        "seq_length = version1_compat_tensorflow.shape(CnnEmbeddedSummary)[1]\n",
        "batch_size = version1_compat_tensorflow.shape(CnnEmbeddedSummary)[0] \n",
        "\n",
        "i=0\n",
        "CNNHidden=version1_compat_tensorflow.zeros([batch_size, hid_lay_cnn], dtype=version1_compat_tensorflow.float32)\n",
        "CLNcell=version1_compat_tensorflow.zeros([batch_size, hid_lay_cnn], dtype=version1_compat_tensorflow.float32)\n",
        "CnnHiddenForward=version1_compat_tensorflow.TensorArray(size=seq_length, dtype=version1_compat_tensorflow.float32)\n",
        "\n",
        "CnnEmbeddedSummary_t = version1_compat_tensorflow.transpose(CnnEmbeddedSummary,[1,0,2]) \n",
        "\n",
        "\n",
        "def conditonsForward(i, CNNHidden, CLNcell, CnnHiddenForward):\n",
        "    return i < seq_length\n",
        "\n",
        "def bodyForward(i, CNNHidden, CLNcell, CnnHiddenForward):\n",
        "    x = CnnEmbeddedSummary_t[i]\n",
        "    \n",
        "    CNNHidden,CLNcell = CnnLstm(x,CNNHidden,CLNcell,embedding_dimensions,hid_lay_cnn,scope=\"forward_encoder\")\n",
        "    CnnHiddenForward = CnnHiddenForward.write(i, CNNHidden)\n",
        "\n",
        "    return i+1, CNNHidden, CLNcell, CnnHiddenForward\n",
        "\n",
        "_, _, _, CnnHiddenForward = version1_compat_tensorflow.while_loop(conditonsForward, bodyForward, [i, CNNHidden, CLNcell, CnnHiddenForward])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXsE5mngYXHH"
      },
      "outputs": [],
      "source": [
        "# defining architecture for backward encoding\n",
        "i=seq_length-1\n",
        "CnnHidden=version1_compat_tensorflow.zeros([batch_size, hid_lay_cnn], dtype=version1_compat_tensorflow.float32)\n",
        "CLNcell=version1_compat_tensorflow.zeros([batch_size, hid_lay_cnn], dtype=version1_compat_tensorflow.float32)\n",
        "CnnHiddenBackward=version1_compat_tensorflow.TensorArray(size=seq_length, dtype=version1_compat_tensorflow.float32)\n",
        "\n",
        "def conditonsBackward(i, CnnHidden, CLNcell, CnnHiddenBackward):\n",
        "    return i >= 0\n",
        "\n",
        "def bodyBackward(i, CnnHidden, CLNcell, CnnHiddenBackward):\n",
        "    x = CnnEmbeddedSummary_t[i]\n",
        "    CnnHidden,cell = CnnLstm(x,CnnHidden,CLNcell,embedding_dimensions,hid_lay_cnn,scope=\"backward_encoder\")\n",
        "    CnnHiddenBackward = CnnHiddenBackward.write(i,CnnHidden)\n",
        "\n",
        "    return i-1, CnnHidden, CLNcell, CnnHiddenBackward\n",
        "\n",
        "_, _, _, CnnHiddenBackward = version1_compat_tensorflow.while_loop(conditonsBackward, bodyBackward, [i, CnnHidden, CLNcell, CnnHiddenBackward])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhkEFvpZYZQb"
      },
      "outputs": [],
      "source": [
        "# merging forward and backward hidden states\n",
        "CnnHiddenForward = CnnHiddenForward.stack()\n",
        "CnnHiddenBackward = CnnHiddenBackward.stack()\n",
        "\n",
        "CnnEncoderStates = version1_compat_tensorflow.concat([CnnHiddenForward,CnnHiddenBackward],axis=-1)\n",
        "CnnEncoderStates = version1_compat_tensorflow.transpose(CnnEncoderStates,[1,0,2])\n",
        "\n",
        "CnnEncoderStates = CnnDropout(CnnEncoderStates,rate=0.3,training=train_tensor_cnn)\n",
        "\n",
        "FinalEncodedState = CnnDropout(version1_compat_tensorflow.concat([CnnHiddenForward[-1],CnnHiddenBackward[-1]],axis=-1),rate=0.3,training=train_tensor_cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ2GG4IkYbZ1"
      },
      "outputs": [],
      "source": [
        "# defining architecture for attention scoring\n",
        "def CnnAttentionScore(CnnEncoderStates,CnnDecoderHiddenState,scope=\"attention_score\"):\n",
        "    \n",
        "    with version1_compat_tensorflow.variable_scope(scope,reuse=version1_compat_tensorflow.AUTO_REUSE):\n",
        "        WeightA = version1_compat_tensorflow.get_variable(\"Wa\", shape=[2*hid_lay_cnn,2*hid_lay_cnn],\n",
        "                                    dtype=version1_compat_tensorflow.float32,\n",
        "                                    trainable=True,\n",
        "                                    initializer=version1_compat_tensorflow.glorot_uniform_initializer())\n",
        "        \n",
        "    CnnEncoderStates = version1_compat_tensorflow.reshape(CnnEncoderStates,[batch_size*seq_length,2*hid_lay_cnn])\n",
        "    \n",
        "    CnnEncoderStates = version1_compat_tensorflow.reshape(version1_compat_tensorflow.matmul(CnnEncoderStates,WeightA),[batch_size,seq_length,2*hid_lay_cnn])\n",
        "    CnnDecoderHiddenState = version1_compat_tensorflow.reshape(CnnDecoderHiddenState,[batch_size,2*hid_lay_cnn,1])\n",
        "    \n",
        "    return version1_compat_tensorflow.reshape(version1_compat_tensorflow.matmul(CnnEncoderStates,CnnDecoderHiddenState),[batch_size,seq_length])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2n0WV_LgYdW9"
      },
      "outputs": [],
      "source": [
        "# defining function for local attention\n",
        "def align(CnnEncoderStates, CnnDecoderHiddenState,scope=\"attention\"):\n",
        "    \n",
        "    with version1_compat_tensorflow.variable_scope(scope,reuse=version1_compat_tensorflow.AUTO_REUSE):\n",
        "        W_par = version1_compat_tensorflow.get_variable(\"Wp\", shape=[2*hid_lay_cnn,128],\n",
        "                                    dtype=version1_compat_tensorflow.float32,\n",
        "                                    trainable=True,\n",
        "                                    initializer=version1_compat_tensorflow.glorot_uniform_initializer())\n",
        "        \n",
        "        V_par = version1_compat_tensorflow.get_variable(\"Vp\", shape=[128,1],\n",
        "                            dtype=version1_compat_tensorflow.float32,\n",
        "                            trainable=True,\n",
        "                            initializer=version1_compat_tensorflow.glorot_uniform_initializer())\n",
        "    \n",
        "    CnnPositionsVector = version1_compat_tensorflow.cast(seq_length-winDowCNNSize,dtype=version1_compat_tensorflow.float32)\n",
        " \n",
        "    Cnnpsv = CnnPositionsVector*version1_compat_tensorflow.nn.sigmoid(version1_compat_tensorflow.matmul(version1_compat_tensorflow.tanh(version1_compat_tensorflow.matmul(CnnDecoderHiddenState,W_par)),V_par))\n",
        "    Cnnptv = Cnnpsv+Attention_Dimension_Size \n",
        "    Cnnptv = version1_compat_tensorflow.reshape(Cnnptv,[batch_size])\n",
        "    \n",
        "    i = 0\n",
        "    CnnGausPosScoreBest = version1_compat_tensorflow.TensorArray(size=seq_length,dtype=version1_compat_tensorflow.float32)\n",
        "    sigma = version1_compat_tensorflow.constant(Attention_Dimension_Size/2,dtype=version1_compat_tensorflow.float32)\n",
        "    \n",
        "    def cond(i,CnnGausPosScoreBest):\n",
        "        \n",
        "        return i < seq_length\n",
        "                      \n",
        "    def body(i,CnnGausPosScoreBest):\n",
        "        \n",
        "        score = version1_compat_tensorflow.exp(-((version1_compat_tensorflow.square(version1_compat_tensorflow.cast(i,version1_compat_tensorflow.float32)-Cnnptv))/(2*version1_compat_tensorflow.square(sigma)))) \n",
        "        CnnGausPosScoreBest = CnnGausPosScoreBest.write(i,score)\n",
        "            \n",
        "        return i+1,CnnGausPosScoreBest\n",
        "                      \n",
        "    i,CnnGausPosScoreBest = version1_compat_tensorflow.while_loop(cond,body,[i,CnnGausPosScoreBest])\n",
        "    \n",
        "    CnnGausPosScoreBest = CnnGausPosScoreBest.stack()\n",
        "    CnnGausPosScoreBest = version1_compat_tensorflow.transpose(CnnGausPosScoreBest,[1,0])\n",
        "    CnnGausPosScoreBest = version1_compat_tensorflow.reshape(CnnGausPosScoreBest,[batch_size,seq_length])\n",
        "    \n",
        "    AttCNNScores = CnnAttentionScore(CnnEncoderStates,CnnDecoderHiddenState)*CnnGausPosScoreBest\n",
        "    AttCNNScores = version1_compat_tensorflow.nn.softmax(AttCNNScores,axis=-1)\n",
        "    \n",
        "    return version1_compat_tensorflow.reshape(AttCNNScores,[batch_size,seq_length,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqtWIlOiYhbV"
      },
      "outputs": [],
      "source": [
        "# defining architecture for lstm decoder with local attention\n",
        "with version1_compat_tensorflow.variable_scope(\"decoder\",reuse=version1_compat_tensorflow.AUTO_REUSE):\n",
        "    starting_marker = version1_compat_tensorflow.get_variable(\"sos\", shape=[1,embedding_dimensions],\n",
        "                                dtype=version1_compat_tensorflow.float32,\n",
        "                                trainable=True,\n",
        "                                initializer=version1_compat_tensorflow.glorot_uniform_initializer())\n",
        "\n",
        "    \n",
        "    WeightC = version1_compat_tensorflow.get_variable(\"Wc\", shape=[4*hid_lay_cnn,embedding_dimensions],\n",
        "                            dtype=version1_compat_tensorflow.float32,\n",
        "                            trainable=True,\n",
        "                            initializer=version1_compat_tensorflow.glorot_uniform_initializer())\n",
        "    \n",
        "\n",
        "\n",
        "starting_marker = version1_compat_tensorflow.tile(starting_marker,[batch_size,1])\n",
        "inputIteratorMarker = starting_marker\n",
        "hiddenStateLSTM=FinalEncodedState\n",
        "CLNcell=version1_compat_tensorflow.zeros([batch_size, 2*hid_lay_cnn], dtype=version1_compat_tensorflow.float32)\n",
        "CnnLstmDecoderOutput=version1_compat_tensorflow.TensorArray(size=MaxSummaryLen_CNN, dtype=version1_compat_tensorflow.float32)\n",
        "outputs=version1_compat_tensorflow.TensorArray(size=MaxSummaryLen_CNN, dtype=version1_compat_tensorflow.int32)\n",
        "\n",
        "CnnAttentionAttCNNScores = align(CnnEncoderStates,hiddenStateLSTM)\n",
        "encoder_context_vector = version1_compat_tensorflow.reduce_sum(CnnEncoderStates*CnnAttentionAttCNNScores,axis=1)\n",
        "\n",
        "for iter in range(MaxSummaryLen_CNN):\n",
        "    \n",
        "    inputIteratorMarker  = CnnDropout(inputIteratorMarker ,rate=0.3,training=train_tensor_cnn)\n",
        "    \n",
        "    inputIteratorMarker  = version1_compat_tensorflow.concat([inputIteratorMarker ,encoder_context_vector],axis=-1)\n",
        "    \n",
        "    hiddenStateLSTM,CLNcell = CnnLstm(inputIteratorMarker ,hiddenStateLSTM,CLNcell,embedding_dimensions+2*hid_lay_cnn,2*hid_lay_cnn,scope=\"decoder\")\n",
        "    \n",
        "    hiddenStateLSTM = CnnDropout(hiddenStateLSTM,rate=0.3,training=train_tensor_cnn)\n",
        "    \n",
        "    CnnAttentionAttCNNScores = align(CnnEncoderStates,hiddenStateLSTM)\n",
        "    encoder_context_vector = version1_compat_tensorflow.reduce_sum(CnnEncoderStates*CnnAttentionAttCNNScores,axis=1)\n",
        "    \n",
        "    concated = version1_compat_tensorflow.concat([hiddenStateLSTM,encoder_context_vector],axis=-1)\n",
        "    \n",
        "    linear_out = version1_compat_tensorflow.nn.tanh(version1_compat_tensorflow.matmul(concated,WeightC))\n",
        "    CnnDecoderOutput = version1_compat_tensorflow.matmul(linear_out,version1_compat_tensorflow.transpose(embed_tensor_cnn,[1,0])) \n",
        "\n",
        "    \n",
        "    CnnLstmDecoderOutput = CnnLstmDecoderOutput.write(iter,CnnDecoderOutput)\n",
        "    \n",
        "    \n",
        "    next_word_vec = version1_compat_tensorflow.cast(version1_compat_tensorflow.argmax(CnnDecoderOutput,1),version1_compat_tensorflow.int32)\n",
        "\n",
        "    next_word_vec = version1_compat_tensorflow.reshape(next_word_vec, [batch_size])\n",
        "\n",
        "    outputs = outputs.write(iter,next_word_vec)\n",
        "\n",
        "    next_word = version1_compat_tensorflow.nn.embedding_lookup(embed_tensor_cnn, next_word_vec)\n",
        "    inputIteratorMarker  = version1_compat_tensorflow.reshape(next_word, [batch_size, embedding_dimensions])\n",
        "    \n",
        "    \n",
        "CnnLstmDecoderOutput = CnnLstmDecoderOutput.stack()\n",
        "outputs = outputs.stack()\n",
        "\n",
        "CnnLstmDecoderOutput = version1_compat_tensorflow.transpose(CnnLstmDecoderOutput,[1,0,2])\n",
        "outputs = version1_compat_tensorflow.transpose(outputs,[1,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4kysqRMYlPs"
      },
      "outputs": [],
      "source": [
        "# defining cost function for cross entropy and l2 regularization\n",
        "filtered_trainables = [var for var in version1_compat_tensorflow.trainable_variables() if\n",
        "                       not(\"Bias\" in var.name or \"bias\" in var.name\n",
        "                           or \"noreg\" in var.name)]\n",
        "\n",
        "regularization = version1_compat_tensorflow.reduce_sum([version1_compat_tensorflow.nn.l2_loss(var) for var\n",
        "                                in filtered_trainables])\n",
        "\n",
        "with version1_compat_tensorflow .variable_scope(\"loss\"):\n",
        "\n",
        "    epsilon = version1_compat_tensorflow .constant(1e-9, version1_compat_tensorflow.float32)\n",
        "\n",
        "    crossEntropy = version1_compat_tensorflow .nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        labels=sumCNN_Tensor, logits=CnnLstmDecoderOutput)\n",
        "\n",
        "    MaskPadCnn = version1_compat_tensorflow.sequence_mask(summary_true_tensor_len_cnn,\n",
        "                                maxlen=MaxSummaryLen_CNN,\n",
        "                                dtype=version1_compat_tensorflow .float32)\n",
        "\n",
        "    CnnmaskedCrossEntropy = crossEntropy*MaskPadCnn\n",
        "\n",
        "    costCnn = version1_compat_tensorflow .reduce_mean(CnnmaskedCrossEntropy) + \\\n",
        "        regL2Param*regularization\n",
        "\n",
        "    cross_entropy = version1_compat_tensorflow .reduce_mean(CnnmaskedCrossEntropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRnQJjjKYonF"
      },
      "outputs": [],
      "source": [
        "# compare label and predicted sequence\n",
        "comparison = version1_compat_tensorflow.cast(version1_compat_tensorflow.equal(outputs, sumCNN_Tensor),\n",
        "                     version1_compat_tensorflow.float32)\n",
        "# Ignore effect of pads by masking\n",
        "\n",
        "MaskPadCnn = version1_compat_tensorflow.sequence_mask(summary_true_tensor_len_cnn,\n",
        "                            maxlen=MaxSummaryLen_CNN,\n",
        "                            dtype=version1_compat_tensorflow.bool)\n",
        "\n",
        "masked_comparison = version1_compat_tensorflow.boolean_mask(comparison,MaskPadCnn)\n",
        "#Getting accuracy\n",
        "CnnAccRacy = version1_compat_tensorflow.reduce_mean(masked_comparison)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pY5erKImYqfb"
      },
      "outputs": [],
      "source": [
        "variableAllTensor = version1_compat_tensorflow.get_collection(version1_compat_tensorflow.GraphKeys.TRAINABLE_VARIABLES)\n",
        "\n",
        "CnnOptimizer = version1_compat_tensorflow.train.AdamOptimizer(learning_rate=learnRate_cnn)\n",
        "\n",
        "gradientVarCnn = CnnOptimizer.compute_gradients(costCnn, variableAllTensor)\n",
        "\n",
        "cappedGradientVarCnn = [(version1_compat_tensorflow.clip_by_norm(gradientCnn, 5), variableCnn) for gradientCnn, variableCnn in gradientVarCnn] \n",
        "\n",
        "OperationTrain = CnnOptimizer.apply_gradients(cappedGradientVarCnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li00xUSdYsbT",
        "outputId": "d1d2a871-b433-4df9-b4c6-9e0d8e9c3817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Training Begins\n",
            "\n",
            "\n",
            "Iter 0, Cost Computed = 1.925, Accurracy Computed = 0.00%\n",
            "Iter 100, Cost Computed = 0.962, Accurracy Computed = 33.51%\n",
            "Iter 200, Cost Computed = 0.828, Accurracy Computed = 36.05%\n",
            "Iter 300, Cost Computed = 0.644, Accurracy Computed = 41.46%\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import random\n",
        "\n",
        "with version1_compat_tensorflow.Session() as CnnSession:  \n",
        "    displayStep = 100\n",
        "    CnnPatience = 5\n",
        "\n",
        "    load = input(\"\\nLoad through previous ckpt file ? y/n: \")\n",
        "    print(\"\")\n",
        "    saver = version1_compat_tensorflow.train.Saver()\n",
        "\n",
        "    if load.lower() == 'y':\n",
        "\n",
        "\n",
        "        saver.restore(CnnSession, 'CNN_summary.ckpt')\n",
        "        CnnSession.run(version1_compat_tensorflow.global_variables())\n",
        "        CnnSession.run(version1_compat_tensorflow.tables_initializer())\n",
        "\n",
        "        with open('CNN_summary.pkl', 'rb') as fp:\n",
        "            train_data = pickle.load(fp)\n",
        "\n",
        "        epochsCovered = train_data['epochsCovered']\n",
        "        bestLoss = train_data['best_loss']\n",
        "        CnnImp = 0\n",
        "        \n",
        "\n",
        "    else:\n",
        "        bestLoss = 2**30\n",
        "        CnnImp = 0\n",
        "        epochsCovered = 0\n",
        "\n",
        "        initial_state = version1_compat_tensorflow.global_variables_initializer()\n",
        "        CnnSession.run(initial_state)\n",
        "        CnnSession.run(version1_compat_tensorflow.tables_initializer())\n",
        "\n",
        "    CnnEpoch=0\n",
        "    while (CnnEpoch+epochsCovered)<epochs:\n",
        "        \n",
        "        print(\"\\n\\nTraining Begins\\n\\n\")\n",
        "        \n",
        "        batches_indices = [i for i in range(0, len(train_article_batches))]\n",
        "        random.shuffle(batches_indices)\n",
        "\n",
        "        SumTracAcc = 0\n",
        "        SumTrainloss = 0\n",
        "\n",
        "        for i in range(0, len(train_article_batches)):\n",
        "            \n",
        "            j = int(batches_indices[i])\n",
        "\n",
        "            costCnn,prediction,\\\n",
        "                acc, _ = CnnSession.run([cross_entropy,\n",
        "                                   outputs,\n",
        "                                   CnnAccRacy,\n",
        "                                   OperationTrain],\n",
        "                                  feed_dict={article_tensor_cnn: train_article_batches[j],\n",
        "                                             embed_tensor_cnn: embd,\n",
        "                                             sumCNN_Tensor: train_summary_batches[j],\n",
        "                                             summary_true_tensor_len_cnn: train_summary_batches_summary_length[j],\n",
        "                                             train_tensor_cnn: True})\n",
        "            \n",
        "            SumTracAcc += acc\n",
        "            SumTrainloss += costCnn\n",
        "\n",
        "            if i % displayStep == 0:\n",
        "                print(\"Iter \"+str(i)+\", Cost Computed = \" +\n",
        "                      \"{:.3f}\".format(costCnn)+\", Accurracy Computed = \" +\n",
        "                      \"{:.2f}%\".format(acc*100))\n",
        "            \n",
        "            if i % 500 == 0:\n",
        "                \n",
        "                idx = random.randint(0,len(train_article_batches[j])-1)\n",
        "                \n",
        "                \n",
        "                \n",
        "                text = \" \".join([index_vocabulary_mapping.get(vec,\"<UNK>\") for vec in train_article_batches[j][idx]])\n",
        "                predicted_summary = [index_vocabulary_mapping.get(vec,\"<UNK>\") for vec in prediction[idx]]\n",
        "                actual_summary = [index_vocabulary_mapping.get(vec,\"<UNK>\") for vec in train_summary_batches[j][idx]]\n",
        "                \n",
        "                \n",
        "        print(\"\\n\\nPerforming checks\\n\\n\")\n",
        "                \n",
        "        valTotalLoss=0\n",
        "        valTotalAccuracy=0\n",
        "                \n",
        "        for i in range(0, len(validation_article_batches)):\n",
        "            \n",
        "            if i%100==0:\n",
        "                print(\"Validating data # {}\".format(i))\n",
        "\n",
        "            costCnn, prediction,\\\n",
        "                acc = CnnSession.run([cross_entropy,\n",
        "                                outputs,\n",
        "                                CnnAccRacy],\n",
        "                                  feed_dict={article_tensor_cnn: validation_article_batches[i],\n",
        "                                             embed_tensor_cnn: embd,\n",
        "                                             sumCNN_Tensor: validation_summary_batches[i],\n",
        "                                             summary_true_tensor_len_cnn: validation_summary_batches_summary_length[i],\n",
        "                                             train_tensor_cnn: False})\n",
        "            \n",
        "            valTotalLoss += costCnn\n",
        "            valTotalAccuracy += acc\n",
        "            \n",
        "        avg_val_loss = valTotalLoss/len(validation_article_batches)\n",
        "        \n",
        "        print(\"\\n Epochs covered : {}\\n\\n\".format(CnnEpoch+epochsCovered))\n",
        "        print(\"Avg Train Loss: {:.3f}\".format(SumTrainloss/len(train_article_batches)))\n",
        "        print(\"Avg Train Accuracy: {:.2f}\".format(100*SumTracAcc/len(train_article_batches)))\n",
        "        print(\"Avg Val Loss: {:.3f}\".format(avg_val_loss))\n",
        "        print(\"Avg Val Acc: {:.2f}\".format(100*valTotalAccuracy/len(validation_article_batches)))\n",
        "              \n",
        "        if (avg_val_loss < bestLoss):\n",
        "            best_loss = avg_val_loss\n",
        "            save_data={'best_loss':best_loss,'epochsCovered':epochsCovered+CnnEpoch+1}\n",
        "            CnnImp=0\n",
        "            with open('CNN_summary.pkl', 'wb') as fp:\n",
        "                pickle.dump(save_data, fp)\n",
        "            saver.save(CnnSession, 'CNN_summary.ckpt')\n",
        "            print(\"\\nModel saved\\n\")\n",
        "              \n",
        "        else:\n",
        "            CnnImp+=1\n",
        "              \n",
        "        if CnnImp > CnnPatience:\n",
        "              break\n",
        "              \n",
        "              \n",
        "        CnnEpoch+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh7kloeWY2EK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Final_Summarization_Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}